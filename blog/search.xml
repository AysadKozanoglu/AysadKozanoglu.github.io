<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>LVM raid1 recovery: mount disk on another linux system debian buster</title>
      <link href="/blog/2023/11/28/lvm/raid1/disk/mount/recovery/howto/"/>
      <url>/blog/2023/11/28/lvm/raid1/disk/mount/recovery/howto/</url>
      
        <content type="html"><![CDATA[<h2 id="scenario"><a href="#scenario" class="headerlink" title="scenario"></a>scenario</h2><p>in my showcase, i was using two ssd 2TB disks in LVM in raid1 mode.<br>My system was debian buster (10).</p><p>My Mainboard was crashed and i was not able to boot up my system. My OS  was installed on  seperatly 240GB ssd.</p><p>So my soft raid1 with LVM for the 2 disk with 2TB ssd, i was not able to get my data from there.</p><ul><li>240gb OS # only OS</li><li>2TB LVM Raid1 disk1 # data</li><li>2TB LVM Raid1 disk2 # data</li></ul><p>Here is my solution for recovery data from LVM raid1 from one part of disk from lvm raid1</p><h3 id="remove-disk-from-damaged-hardware"><a href="#remove-disk-from-damaged-hardware" class="headerlink" title="remove disk from  damaged hardware"></a>remove disk from  damaged hardware</h3><p>remove the one part of the ssd disk to connect with usb ssd adapter (sabrent) to another system. in my case i  working with debian buster (10)</p><h3 id="connect-disk-to-another-working-system"><a href="#connect-disk-to-another-working-system" class="headerlink" title="connect disk to another working system"></a>connect disk to another working system</h3><p>connect the disk with usb ssd adapter to your another working machine. in my case debian buster (10)</p><h2 id="get-info-from-connected-ssd-with-usb"><a href="#get-info-from-connected-ssd-with-usb" class="headerlink" title="get info from connected ssd with usb"></a>get info from connected ssd with usb</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">fdisk -l</span><br><span class="line">pvdisplay</span><br><span class="line">vgdisplay</span><br></pre></td></tr></table></figure><blockquote><p>info: all this commands gives you  output and you would  get  information about disk,  lovume group, partiton volume, VG name, PV name</p></blockquote><p>pvdisplay and vgdisplay output should give you in the first line a WARNING:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">WARNING: Couldn&#x27;t find device with uuid kGfIXi-i1m2-7pqw-5Aqb-mH2U-Q20f-YMrg1M.</span><br><span class="line">WARNING: VG VG01_STORAGE_RAID1 is missing PV kGfIXi-i1m2-7pqw-5Aqb-mH2U-Q20f-YMrg1M (last written to /dev/sdc).</span><br><span class="line"></span><br><span class="line">  --- Physical volume ---</span><br><span class="line">  PV Name               [unknown]</span><br><span class="line">  VG Name               VG01_STORAGE_RAID1</span><br><span class="line">  PV Size               &lt;1,82 TiB / not usable &lt;1,09 MiB</span><br><span class="line">  Allocatable           yes (but full)</span><br><span class="line">  PE Size               4,00 MiB</span><br><span class="line">  Total PE              476932</span><br><span class="line">  Free PE               0</span><br><span class="line">  Allocated PE          476932</span><br><span class="line">  PV UUID               kGfIXi-i1m2-7pqw-5Aqb-mH2U-Q20f-YMrg1M</span><br><span class="line"></span><br><span class="line">  --- Physical volume ---</span><br><span class="line">  PV Name               /dev/sdc</span><br><span class="line">  VG Name               VG01_STORAGE_RAID1</span><br><span class="line">  PV Size               &lt;1,82 TiB / not usable &lt;1,09 MiB</span><br><span class="line">  Allocatable           yes (but full)</span><br><span class="line">  PE Size               4,00 MiB</span><br><span class="line">  Total PE              476932</span><br><span class="line">  Free PE               0</span><br><span class="line">  Allocated PE          476932</span><br><span class="line">  PV UUID               O0cX9Z-3YD8-dkFX-qPCA-i6RS-fAGe-Nbplt6</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><blockquote><p>info: the is valid and it is ok because the partial disk (second disk) is not foundable </p></blockquote><h2 id="mount-lvm-raid1-partial-disk"><a href="#mount-lvm-raid1-partial-disk" class="headerlink" title="mount lvm raid1 partial disk"></a>mount lvm raid1 partial disk</h2><p>first, scan all connected disk for LVM with lvm tool lvscan</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">lvscan</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>output:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ACTIVE &#x27;/dev/VG01_STORAGE_RAID1/LV01_STORAGE_RAID1&#x27; [&lt;1,82 TiB] inherit</span><br></pre></td></tr></table></figure><blockquote><p>info: in my scenario, i  named my volume group  <em>VG01_STORAGE_RAID1&#x2F;LV01_STORAGE_RAID1</em> . this part of disk is not mountable, until i removed the missing part disk (second disk &gt; raid1) </p></blockquote><h2 id="removing-missing-partial-disk-from-LVM-group"><a href="#removing-missing-partial-disk-from-LVM-group" class="headerlink" title="removing missing partial disk from LVM group"></a>removing missing partial disk from LVM group</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">vgreduce --removemissing --force VG01_STORAGE_RAID1</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>output:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Couldn&#x27;t find device with uuid kGfIXi-i1m2-7pqw-5Aqb-mH2U-Q20f-YMrg1M.</span><br><span class="line">Removing partial LV LV01_STORAGE_RAID1.</span><br><span class="line">Logical volume &quot;LV01_STORAGE_RAID1&quot; successfully removed</span><br><span class="line">Wrote out consistent volume group VG01_STORAGE_RAID1</span><br></pre></td></tr></table></figure><blockquote><p>info: failed&#x2F;bad disk is now successfully removed from LVM group</p></blockquote><h2 id="get-UUID-from-partial-VG-disk"><a href="#get-UUID-from-partial-VG-disk" class="headerlink" title="get UUID from partial VG disk"></a>get UUID from partial VG disk</h2><p>to rename VG volume group we have to get VG UUID first.</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vgdisplay</span><br></pre></td></tr></table></figure><p>output:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">--- Volume group ---</span><br><span class="line">VG Name               VG01_STORAGE_RAID1</span><br><span class="line">System ID</span><br><span class="line">Format                lvm2</span><br><span class="line">Metadata Areas        1</span><br><span class="line">Metadata Sequence No  3</span><br><span class="line">VG Access             read/write</span><br><span class="line">VG Status             resizable</span><br><span class="line">MAX LV                0</span><br><span class="line">Cur LV                1</span><br><span class="line">Open LV               0</span><br><span class="line">Max PV                0</span><br><span class="line">Cur PV                2</span><br><span class="line">Act PV                1</span><br><span class="line">VG Size               &lt;3,64 TiB</span><br><span class="line">PE Size               4,00 MiB</span><br><span class="line">Total PE              953864</span><br><span class="line">Alloc PE / Size       953864 / &lt;3,64 TiB</span><br><span class="line">Free  PE / Size       0 / 0</span><br><span class="line">VG UUID               WNR45t-kuju-bSZq-iF2f-LJoA-ZCdb-G2uDkl</span><br></pre></td></tr></table></figure><blockquote><p>info: VG UUID is WNR45t-kuju-bSZq-iF2f-LJoA-ZCdb-G2uDkl</p></blockquote><h2 id="rename-VG-volume-group-name"><a href="#rename-VG-volume-group-name" class="headerlink" title="rename VG volume group name"></a>rename VG volume group name</h2><p>to mount as single disk we have to rename the LVM VG name first</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">#vgrename &lt;UUID&gt;  NEW_NAME</span><br><span class="line">vgrename WNR45t-kuju-bSZq-iF2f-LJoA-ZCdb-G2uDkl NEW_VG01_STORAGE_RAID1</span><br><span class="line">vgchange -ay</span><br><span class="line">lvscan</span><br><span class="line"></span><br><span class="line">$#: ACTIVE &#x27;/dev/NEW_VG01_STORAGE_RAID1/LV01_STORAGE_RAID1&#x27; [&lt;1,82 TiB] inherit</span><br></pre></td></tr></table></figure><blockquote><p>info: now VG name is renamed to NEW_VG01_STORAGE_RAID1</p></blockquote><h2 id="mount-LVM-to-mountpoint"><a href="#mount-LVM-to-mountpoint" class="headerlink" title="mount LVM to mountpoint"></a>mount LVM to mountpoint</h2><h3 id="enable-dm-mod-in-kernel"><a href="#enable-dm-mod-in-kernel" class="headerlink" title="enable dm-mod in kernel"></a>enable dm-mod in kernel</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">modprobe dm-mod</span><br></pre></td></tr></table></figure><h3 id="create-mountpoint"><a href="#create-mountpoint" class="headerlink" title="create mountpoint"></a>create mountpoint</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mkdir /mnt/LVM-RECOVED-DISK1</span><br></pre></td></tr></table></figure><h3 id="mount-LVM-DISK-VG"><a href="#mount-LVM-DISK-VG" class="headerlink" title="mount LVM DISK VG"></a>mount LVM DISK VG</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mount /dev/NEW_VG01_STORAGE_RAID1/LV01_STORAGE_RAID1 /mnt/LVM-RECOVED-DISK1</span><br></pre></td></tr></table></figure><h2 id="recover-your-data"><a href="#recover-your-data" class="headerlink" title="recover your data"></a>recover your data</h2><p>now you are able to force your data intire <em>&#x2F;mnt&#x2F;LVM-RECOVED-DISK1</em></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd /mnt/LVM-RECOVED-DISK1</span><br><span class="line">ls -la</span><br></pre></td></tr></table></figure><p>thanks for reading , i hope i helped you</p><p>â€“<br>Aysad Kozanoglu</p>]]></content>
      
      
      <categories>
          
          <category> lvm </category>
          
          <category> raid1 </category>
          
          <category> disk </category>
          
          <category> mount </category>
          
          <category> recovery </category>
          
      </categories>
      
      
        <tags>
            
            <tag> lvm </tag>
            
            <tag> recovery </tag>
            
            <tag> disk </tag>
            
            <tag> mount </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
